---
layout: post
title: 英语表述
category: 读书
tag: React
keywords: notes
---

## 表述

1. For each UAP, we evaluate its fooling rate on the model it was generated from (white-box attack) and on the three remaining models (transfer attack).
2. Our results show that models trained on Stylized-ImageNet are still **as vulnerable to** these UAPs **as** models trained on ImageNet.
3. For transfer attacks, where the UAP is generated from a model different from the evaluated one, the fooling rate consistently rises for " > 4.
4. Particularly, in case of critical applications that involve safety and security, reliable models need to be deployed to stand against the strong adversarial attacks. Thus, the effect of these structured perturbations has to be studied thoroughly in order to develop dependable machine learning systems.  需要研究对抗样本的原因
5. We train G in order to be able to generate the UAPs that can fool the target classifier over the underlying data distribution.
6. Hence**, diagonal entries** denote the white-box adversarial attacks and the off diagonal entries denote the black-box attacks.
7. It computes the gradient of the loss function with respect to pixels, and moves a single step based on the sign of the gradient.
8. To verify this latter claim
9. universal perturbations computed for the VGG-19 network have a fooling ratio above 53% for all other tested architectures.
10. Adversarial samples also possess the characteristic of transferability, which means adversarial sample generated for attacking one model could also mislead another model.
11. In this paper we present the first comprehensive evaluation of transferability of evasion and poisoning availability attacks,
12. as formulated in [42] and in follow-up work (e.g., [33]).
13. It is not difficult to see that， for  ...
14. In Fig. 6a we report the mean test error at e=1 for each target model against the size of its input gradients (S, averaged on the test samples and on the 10 runs).
15. A visual **inspection** of the poisoning digits  中毒数字的目视检查
16. The value in the i-th row and j-th column of each heatmap matrix is the proportion of the adversarial examples successfully transferred to target model j out of all adversarial examples generated by source model i (including both successful and failed attacks on the source model). 混淆矩阵的解释
17. In order to assess texture and shape biases, we conducted six major experiments **along with** three control experiments, which are described in the Appendix.
18. **more fine-grained statement** 
19. We view **bridging this gap** as an interesting direction for future work.
20. We focus on fog in this work, due to the availability of relevant data, but our framework is generalizable and can be easily extended to other weather conditions.)
21. We experimentally analyze Vision Transformers to answer this question.
22. We **urge interested readers to examine the supplemental materia**l where we provide descriptions of each attack.  引用他们文章或意见时，推荐读者去看论文原文的写法
23. We demonstrate the SAGA results by attacking a simple ensemble of Vision Transformers and Big Transfer Models for CIFAR-10, CIFAR-100 and ImageNet. 针对不同数据，模型的写法
24. For ImageNet, we use Bit-M-R152x4 and ViT-L-16
25. This intriguing phenomenon of human imperceptible perturbation fooling the DNN has inspired active research for studying the model robustness against adversarial attack techniques
26. has introduced an efficient one-step attack method, widely known as the Fast Gradient Sign Method (FGSM). [27] has extended the basic FGSM to its iterative variant, i.e. I-FGSM,
27. For consistency we follow prior works [38, 46, 41] and adopt l∞ = 10/255. Unless otherwise specified, the UAP in this work is by default untargeted
28. Concurrent to [46], [20] adopts a similar approach.
29. **Our results align well with** [61] that identifies frequency as a factor for the classwise robustness gap against targeted UAP
30. Overall, **there is a general consensus** among the UAP researchers that crafting UAP with limited or no data is challenging.
31. For a detailed description of their various methods, **we refer the readers to** [31].
32. Universal adversarial perturbation (UAP), i.e. a single perturbation to fool the network for most images, is widely recognized as a more practical attack because the UAP can be generated beforehand and applied directly during the attack stage.  通用对抗扰动的描述
33. image-agnostic perturbations that cause most natural images to be misclassified.
34. Universal adversarial perturbations exhibit many interesting properties such as their
    universality across networks, which means that a perturbation constructed using one DNN will perform relatively well for other DNNs.
35. Our analysis with UAPs reveals the extent to which increased shape-bias improves adversarial robustness of models to universal attacks.  我们对 UAP 的分析揭示了增加的形状偏差在多大程度上提高了模型对普遍攻击的对抗性鲁棒性。
36. According to our experiments, a small number K, for example, k = 2 or 3, will be sufficient.
37. we **omit the detailed description of the model architecture** and **refer the readers to**
38. The PASCAL VOC dataset contains 20 object categories. The majority of images in the PASCAL VOC dataset have 1 to 5 object instances, on average, 1.4 categories and 2.3 instances per image. The MS COCO dataset contains 80 object categories. Images in this dataset have more object instances, on average, 3.5 categories and 7.7 instances per image.
39. Its core idea is that,
40. With the progress of adversarial attacks，  随着对抗攻击的进展
41. Although many certified adversarial defense methods have reported substantial advances in adversarial robustness, recent works [19, 9, 3] report that the gradient masking may present a false sense of security, i.
42. Despite a large literature devoted to improving the robustness of deep-learning models, many fundamental questions remain unresolved.  虽然有发展，但是还有很多问题待解决
43. The regularization parameter ( is an important hyper-parameter in our proposed method. We show how the regularization parameter affects the performance of our robust classifiers by numerical experiments on two datasets, MNIST and CIFAR10. 某个超参的重要性
44. **The figure shows the classification accuracy rates (percentage) of ResNet against different attacks (higher is better) on CIFAR-10**.
45. **Since the widespread types of attacks in the real world are very diverse**, the adversarial examples used in the training procedure are often biased.
46. the **delicately** crafted special noise  精心打造的特殊噪音
47. However, the existence of adversarial examples **has raised concerns about** the security of computer vision systems
48. To address security concerns for high-stakes applications, **researchers are searching for ways to** make models more robust to attacks
49. Δ is the gradient of y with respect to x:
50. We demonstrate the effectiveness of AdvDrop **by extensive experiments**,
51. The proposed adversarial example detection approach is evaluated against six state-of-the-art adversarial example generation methods: FGSM (L), PGD (L), CW (L), AutoAttack (L), Square (L), and boundary attack. All attacks are implemented as “non-targeted” attacks on three datasets: CIFAR-10 [5], ImageNet dataset [6], and STL- 10 [3].
52. In this setting, the adversary **is aware of** the different steps involved in the adversarial defense method but does not have access to the method’s parameters.
53. The LID and the proposed detection method are trained and tested using the same adversarial attack methods, **except for CWwb attack**, where the detector is trained on the traditional CW attack.
54. Each adversarial example detection method is trained using CW attack, and evaluated on other attacks.
55. **The objective of this experiment** is to investigate the impact of graph topology on detection performance
56. **Our work is motivated by this observation**, and we proposed an algorithm to leverage these robust features.
57. For the DAN algorithm, improvement in robustness is 0% to 22.11% **at the cost of** an 18% drop in clean accuracy. 
58. **Similar improvement in robustness is also visible in** experiments involving Office-31 and Office-Home datasets as shown in Table 1.
59. We choose it based on the magnitude of domain adaptation loss. 对于损失参数设置的解释
60. RFA outperforms both Baseline and Robust Pre-Training **with significant margins**.
61. **The attention mechanism plays a critical role** in human
    visual system and is widely used in a variety of application
    tasks
62. AGKD-BML **overall outperforms** the comparison methods on CIFAR-10.
63. AGKDBML also shows better adversarial robustness on SVHN
    dataset **with a large margin**.
64. The successes achieved in deep learning areas **have made great improvements  for** CMHR.
65. **The lower performance means** better attack capability.
66. we **conduct an extensive amount of experiments to demonstrate the efficacy of** AdvRush on various benchmark
    datasets.
67. there remains one important question that is yet to
    be explored extensively
68. we **warm up**  and ! of the supernet without L. Upon the completion of the warm-up process, we introduce L for additional epochs
69. **Please refer to the Appendix for the visualization of each architecture.**
70. This may **impede their practical deployment for** training robust models and efficiently evaluating robustness.
71. 1We tried to include the B&B ℓ1 and ℓ2 attacks [5] in our comparison, **but their official implementations kept crashing.**
72. Investigating the original code, we **found errors in the implementation** of the CIEDE2000, resulting in both wrong values and wrong gradients.
73. our C&W baseline gets performance that is **on par or better than** ALMA LPIPS on both datasets
74. In recent years, research on adversarial attacks **has become a hot spot**.
75. **Unless mentioned, all experiments in this section are based on** the integration of TI-DIM [9] method with our proposed architecture. 
76. For a comprehensive review, we refer readers to [3]
77. SimCat robustness to adaptive PGD-ℓ2 over epochs of adversarial training **with varied values of the hyperparameter β**, which controls the momentum updates. Higher
78. Note that Y and y are **equivalent notations** of the truth labels of x.
79. **Higher values of ASR** indicate the corresponding method has a high attacking performance.
80. we introduce a new VQA benchmark dynamically collected **with a** Human-and-Model-in-the-Loop **procedure**.
81. natural domain **versus** adversarial domain  自然域与对抗域
82. where small perturbations on the input can easily **subvert** the model’s prediction.
83. **raising safety concerns in** autonomous driving (Eykholt et al., 2018) and medical diagnosis
84. All parameters are determined by grid search.
85. **A natural question arises whether** it is possible to craft a universal perturbation that fools the network only for certain classes **while** having minimal influence on other classes.
86. We quantify **how** the performance benefits of transferring features **decreases** the more dissimilar the base task and target task are.
87. on the spot   立刻，当场；在危险中；处于负责地位
88. **a plausible explanation is that**
89. **Although feature squeezing generalizes to other domains, here we focus on image classification**. Because it is the domain where adversarial examples have been most extensively studied.
90. Their method requires a large set of both adversarial and legitimate inputs and is not capable of detecting individual adversarial examples, making it not useful **in practice**.实际上，事实上；在实践中
91. It is computationally expensive and can only detect adversarial examples lying far from the manifolds of the legitimate population.
92. Given unlimited perturbation bounds, **one could always force a model to misclassify an example into any class**, but often by producing images that are unrecognizable or obviously suspicious to humans.
93. There has been **substantial** work demonstrating that
94. We **perform a systematic empirical study on various defenses.**
95. If these intermediate defender states are “leaked” to the attacker, we call it intermediate defender states leaking, **or simply states leaking**, otherwise we call it non states-leaking, or simply non-leaking.
96. **Countless real-world applications** involve streaming data that arrive in an online fashion
97. Our first insight **elucidates** that
98. **For the sake of convenience**, the names of the networks are shortened as ResNet, Inception, DenseNet, MobileNet, SENet, and PNASNet in the rest of the paper.
99. We use the **publicly available differentiable renderer from** Kato et al. [11] and although they introduced their method for color and silhouettes only, we extend it for rendering depth maps in a differentiable way.
100. **Alongside works endeavoring to explain adversarial examples**, others have proposed defenses in
     order to increase robustness.  除了努力解释对抗性例子的工作外，其他人还提出了防御措施，以提高稳健性。
101. the proposed method achieves **slightly** lower accuracy  on adversarially trained models than BIM
102. owever, these activities **can be viewed as two facets of the same field**, and together they have undergone substantial development over the past ten years.  可以被视为同一领域的两个方面
103. Similar studies between spatial invariance and such common corruptions is **an interesting direction of future work**.
104. **recent efforts have resulted in** equivariant NN models for other transformations such as rotation, flip
105. Such group-equivariant NN models (GCNNs) are designed to **be invariant to** a specific group of transformations,
106. Our approach **results in superior performance as well as intepretability**
107. Previous works **have broadly investigated** the reason for the widespread of such adversarial examples.
108. we construct a simple, **intuitive example** in which shift invariance dramatically reduces robustness. 直观样本
109. **deployed** machine learning systems 已经部署的机器学习系统
110. it is **imperative\necessary\essential\urgent** to ensure that 这是**命令\必要\必要\紧急**以确保
111. this observation **implies** that
112. Some **ascribes** the adversary to linear accumulation of perturbations from inputs to
     final outputs
113. **This phenomenon could be semantically explained** as subclasses from the same superclass share more feature similarities. 这种现象可以在语义上解释为来自同一超类的子类共享更多的特征相似性。
114. **The former and the latter** are often referred to as the whitebox and the blackbox threat
     model, respectively.
115. In particular, the efficacy of the attack is even **higher** than the existing blackbox attacks and **comparable** to the existing whitebox attacks.
116. Further, our method **surpasses** the state-of-the-art whitebox KED-MI attack on Pubfig83
     and achieves a close attack accuracy on the CelebA dataset.
117. In particular, BREP-MI **outperforms** GMI and the blackbox attack **by a substantial margin for all model architectures**.

## 词汇

1. image-agnostic 图像不可知  ==> image-dependent
2. proportional adj. 比例的, 成比例的  the performance of the resulting perturbation is proportional to the available training data.
3. adulterate  vt. (尤指食物)掺假, <废>奸污，诱奸 ,adj. 掺杂的，掺假的；不纯的 ,通奸的，犯通奸罪的 ,n. 掺假
4. susceptible   adj. 易受影响的; 易动感情的, 过敏的; 易受…感染的, 能经受的
5. image-specific 指定图像的
6. image-agnostic  图像不可知论  (universal)
7. obviate vt. 避免,消除(贫困、不方便等)
8. eliminate vt. 消除, 排除,忽略,淘汰〈口〉干掉
9. in lieu of  n. 代替, (以…)替代
10. aggregating 逐个增加
11. conj. 即使；虽然
12. in the vicinity of  在附近
13. precludes  vt. 妨碍；排除；阻止
14. spurious  adj. 假的；伪造的；欺骗的
15. adopts   vt. 收养 ,采用, 采纳, 采取, 正式接受, 通过
16. Thereupon 于是
17. derive  vt. & vi. 得到, 源于
18. encompasses  vt. 围绕;包围
19. curvature  n. 弯曲 弯曲部分 曲率,曲度
20. vice-versa  反之亦然
21. surrogate  n. 替代;代理
22. distinct adj. 截然不同的, 完全分开的 清晰的, 明白的, 明显的 <==> different
23. shed light on 阐明；使…清楚地显出  Through our formalization, we shed light on the most important factors for transferability.
24. **Without loss of generality**, 不失一般性地
25. integrity and availability 完整性和可用性
26. In a nutshell   简而言之
27. posit  vt. 假定,设想,假设
28. corroborate  vt. 证实,支持(某种说法、信仰、理论等) n. 确证者;确证物
29. unmodified adj. 未更改的
30. input-agnostic 输入不可知
31. image-agnostic 图像不可知
32. adulterate vt. (尤指食物)掺假 <废>奸污，诱奸 adj. 掺杂的，掺假的；不纯的 通奸的，犯通奸罪的 n. 掺假
33. To the best of our knowledge
34. in conjunction with  连同，共同, 与…协力
35. uncover 揭露
36. be made up of 由什么组成
37. justified explanation  合理的解释
38. alternative explanation 替代解释
39. ounter-intuitive 违法直觉的
40. If this would be the case 如果是这样的话
41. quasi-imperceptible 不可感知  
42. discrepancy n. 差异，不符合(之处)；不一致(之处)
43. Vanilla model 原始模型(raw)模型
44. a large proportion of  很大一部分
45. contingent  adj. 偶然（发生）的；（损失、责任等）附带的；以事实为依据的；依情况而变的 n. （代表某一组织或国家的）代表团；（军队的）分遣队
46. collaterally   n. 附属担保品  adj. 相关的
47. a spectrum of 连续的, 连串的
48. despite its structural simplicity 尽管结构简单
49. on par with 与...相当
50. the former 前者
51. In the real world, attacks are continuously evolving.  在现实世界中，攻击在不断发展。
52. a toy experiment  玩具实验
53. denominator 分母
54. numerator 分子
55. in an end-to-end manner  以端到端的方式
56. conjecture  n. 推测；猜想 vi. 推测；揣摩 vt. 推测
57. curvature  n. 弯曲，弯曲部分，曲率,曲度
58. derivative  n. 派生物，引出物 adj. 模仿他人的；衍生的；派生的
59. enormous  adj. 巨大的, 极大的, 庞大的
60. subsumed vt. 把……归入；把……包括在内
61. gives rise to 引起、
62. transductive learning 转导学习
63. inductive learning  归纳学习
64. rigorous  adj. 严密的；缜密的，严格的，严厉的
65. sanitize  vt. 使…无害；给…消毒；对…采取卫生措施
66. Transiency 顷刻，无常
67. irrevocable adj. 不可改变的, 不可反转的
68. elucidates  vt. 阐明,解释
69. forego  vt. (在位置时间或程度方面)走在…之前,居先
70. Chiefly adv. 主要地；首先
71. impotent adj. 无力的；无效的；虚弱的；阳萎的
72. Accordingly,   adv. 照着, 相应地 因此, 所以, 于是
73. off-the-shelf  object detectors 表示现有的目标检测器
74. substantial  adj. 坚固的；结实的，大量的，可观的，重大的，重要的，实质的，基本的，大体上的
75. this counter intuitive behavior  这种反直觉行为
76. Even more importantly 更重要的是
77. In the same year
78. insensitive   adj. 感觉迟钝的;不友好的，感觉不到的;麻木不仁的
79. inspect   vt. 检查；视察；检阅 vi. 进行检查；进行视察
80. degrades  vt. 使……丢脸；使……降级；使……降解；贬低 vi. 退化；降级，降低
81. in practice  实际上，事实上；在实践中
82. at present  目前，当前
76.  solve/address  解决问题
76.  indicates,represents,denotes  
76.  imperative /necessary
76.  ascribes 









